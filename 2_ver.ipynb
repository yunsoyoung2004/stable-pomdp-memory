{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75177989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "Epoch 00000 | ep_reward 42.0 | baseline_prev 0.00 -> ema 4.20 | SPIKE | extra_upd=2\n",
      "Epoch 00010 | ep_reward 15.0 | baseline_prev 14.76 -> ema 14.78 | SPIKE | extra_upd=2\n",
      "Epoch 00020 | ep_reward 24.0 | baseline_prev 22.66 -> ema 22.79 | SPIKE | extra_upd=2\n",
      "Epoch 00030 | ep_reward 17.0 | baseline_prev 19.17 -> ema 18.95 | COLLAPSE | extra_upd=2\n",
      "Epoch 00040 | ep_reward 19.0 | baseline_prev 21.55 -> ema 21.30 | COLLAPSE | extra_upd=2\n",
      "Epoch 00050 | ep_reward 11.0 | baseline_prev 25.65 -> ema 24.19 | COLLAPSE | extra_upd=2\n",
      "Epoch 00060 | ep_reward 22.0 | baseline_prev 21.73 -> ema 21.75 | SPIKE | extra_upd=2\n",
      "Epoch 00070 | ep_reward 20.0 | baseline_prev 21.52 -> ema 21.37 | COLLAPSE | extra_upd=2\n",
      "Epoch 00080 | ep_reward 24.0 | baseline_prev 21.72 -> ema 21.95 | SPIKE | extra_upd=2\n",
      "Epoch 00090 | ep_reward 10.0 | baseline_prev 25.10 -> ema 23.59 | COLLAPSE | extra_upd=2\n",
      "Epoch 00100 | ep_reward 10.0 | baseline_prev 19.97 -> ema 18.97 | COLLAPSE | extra_upd=2\n",
      "Epoch 00110 | ep_reward 32.0 | baseline_prev 21.36 -> ema 22.42 | SPIKE | extra_upd=2\n",
      "Epoch 00120 | ep_reward 18.0 | baseline_prev 19.81 -> ema 19.63 | COLLAPSE | extra_upd=2\n",
      "Epoch 00130 | ep_reward 19.0 | baseline_prev 20.96 -> ema 20.76 | COLLAPSE | extra_upd=2\n",
      "Epoch 00140 | ep_reward 10.0 | baseline_prev 26.02 -> ema 24.42 | COLLAPSE | extra_upd=2\n",
      "Epoch 00150 | ep_reward 39.0 | baseline_prev 27.22 -> ema 28.40 | SPIKE | extra_upd=2\n",
      "Epoch 00160 | ep_reward 23.0 | baseline_prev 22.24 -> ema 22.32 | SPIKE | extra_upd=2\n",
      "Epoch 00170 | ep_reward 26.0 | baseline_prev 26.30 -> ema 26.27 | COLLAPSE | extra_upd=2\n",
      "Epoch 00180 | ep_reward 14.0 | baseline_prev 28.81 -> ema 27.33 | COLLAPSE | extra_upd=2\n",
      "Epoch 00190 | ep_reward 36.0 | baseline_prev 26.09 -> ema 27.08 | SPIKE | extra_upd=2\n",
      "Epoch 00200 | ep_reward 13.0 | baseline_prev 24.36 -> ema 23.23 | COLLAPSE | extra_upd=2\n",
      "Epoch 00210 | ep_reward 11.0 | baseline_prev 26.39 -> ema 24.85 | COLLAPSE | extra_upd=2\n",
      "Epoch 00220 | ep_reward 53.0 | baseline_prev 26.53 -> ema 29.18 | SPIKE | extra_upd=2\n",
      "Epoch 00230 | ep_reward 64.0 | baseline_prev 27.38 -> ema 31.04 | SPIKE | extra_upd=2\n",
      "Epoch 00240 | ep_reward 19.0 | baseline_prev 29.16 -> ema 28.14 | COLLAPSE | extra_upd=2\n",
      "Epoch 00250 | ep_reward 15.0 | baseline_prev 25.48 -> ema 24.43 | COLLAPSE | extra_upd=2\n",
      "Epoch 00260 | ep_reward 70.0 | baseline_prev 29.20 -> ema 33.28 | SPIKE | extra_upd=2\n",
      "Epoch 00270 | ep_reward 33.0 | baseline_prev 32.23 -> ema 32.30 | SPIKE | extra_upd=2\n",
      "Epoch 00280 | ep_reward 16.0 | baseline_prev 31.82 -> ema 30.24 | COLLAPSE | extra_upd=2\n",
      "Epoch 00290 | ep_reward 12.0 | baseline_prev 27.81 -> ema 26.23 | COLLAPSE | extra_upd=2\n",
      "Epoch 00300 | ep_reward 11.0 | baseline_prev 28.52 -> ema 26.77 | COLLAPSE | extra_upd=2\n",
      "Epoch 00310 | ep_reward 29.0 | baseline_prev 23.77 -> ema 24.30 | SPIKE | extra_upd=2\n",
      "Epoch 00320 | ep_reward 69.0 | baseline_prev 26.34 -> ema 30.61 | SPIKE | extra_upd=2\n",
      "Epoch 00330 | ep_reward 14.0 | baseline_prev 35.20 -> ema 33.08 | COLLAPSE | extra_upd=2\n",
      "Epoch 00340 | ep_reward 61.0 | baseline_prev 28.45 -> ema 31.71 | SPIKE | extra_upd=2\n",
      "Epoch 00350 | ep_reward 10.0 | baseline_prev 29.27 -> ema 27.34 | COLLAPSE | extra_upd=2\n",
      "Epoch 00360 | ep_reward 19.0 | baseline_prev 24.50 -> ema 23.95 | COLLAPSE | extra_upd=2\n",
      "Epoch 00370 | ep_reward 19.0 | baseline_prev 25.85 -> ema 25.17 | COLLAPSE | extra_upd=2\n",
      "Epoch 00380 | ep_reward 34.0 | baseline_prev 20.54 -> ema 21.89 | SPIKE | extra_upd=2\n",
      "Epoch 00390 | ep_reward 9.0 | baseline_prev 20.50 -> ema 19.35 | COLLAPSE | extra_upd=2\n",
      "Epoch 00400 | ep_reward 36.0 | baseline_prev 23.65 -> ema 24.89 | SPIKE | extra_upd=2\n",
      "Epoch 00410 | ep_reward 21.0 | baseline_prev 27.41 -> ema 26.77 | COLLAPSE | extra_upd=2\n",
      "Epoch 00420 | ep_reward 31.0 | baseline_prev 27.76 -> ema 28.09 | SPIKE | extra_upd=2\n",
      "Epoch 00430 | ep_reward 30.0 | baseline_prev 24.82 -> ema 25.33 | SPIKE | extra_upd=2\n",
      "Epoch 00440 | ep_reward 38.0 | baseline_prev 25.72 -> ema 26.95 | SPIKE | extra_upd=2\n",
      "Epoch 00450 | ep_reward 15.0 | baseline_prev 25.83 -> ema 24.75 | COLLAPSE | extra_upd=2\n",
      "Epoch 00460 | ep_reward 26.0 | baseline_prev 27.30 -> ema 27.17 | COLLAPSE | extra_upd=2\n",
      "Epoch 00470 | ep_reward 47.0 | baseline_prev 32.28 -> ema 33.75 | SPIKE | extra_upd=2\n",
      "Epoch 00480 | ep_reward 34.0 | baseline_prev 28.69 -> ema 29.22 | SPIKE | extra_upd=2\n",
      "Epoch 00490 | ep_reward 25.0 | baseline_prev 32.03 -> ema 31.33 | COLLAPSE | extra_upd=2\n",
      "Epoch 00500 | ep_reward 38.0 | baseline_prev 39.15 -> ema 39.04 | COLLAPSE | extra_upd=2\n",
      "Epoch 00510 | ep_reward 51.0 | baseline_prev 32.14 -> ema 34.03 | SPIKE | extra_upd=2\n",
      "Epoch 00520 | ep_reward 62.0 | baseline_prev 33.92 -> ema 36.72 | SPIKE | extra_upd=2\n",
      "Epoch 00530 | ep_reward 42.0 | baseline_prev 36.23 -> ema 36.81 | SPIKE | extra_upd=2\n",
      "Epoch 00540 | ep_reward 20.0 | baseline_prev 31.29 -> ema 30.16 | COLLAPSE | extra_upd=2\n",
      "Epoch 00550 | ep_reward 35.0 | baseline_prev 32.84 -> ema 33.05 | SPIKE | extra_upd=2\n",
      "Epoch 00560 | ep_reward 24.0 | baseline_prev 35.05 -> ema 33.94 | COLLAPSE | extra_upd=2\n",
      "Epoch 00570 | ep_reward 12.0 | baseline_prev 35.71 -> ema 33.34 | COLLAPSE | extra_upd=2\n",
      "Epoch 00580 | ep_reward 36.0 | baseline_prev 28.86 -> ema 29.57 | SPIKE | extra_upd=2\n",
      "Epoch 00590 | ep_reward 37.0 | baseline_prev 30.62 -> ema 31.26 | SPIKE | extra_upd=2\n",
      "Epoch 00600 | ep_reward 28.0 | baseline_prev 34.74 -> ema 34.07 | COLLAPSE | extra_upd=2\n",
      "Epoch 00610 | ep_reward 15.0 | baseline_prev 37.19 -> ema 34.98 | COLLAPSE | extra_upd=2\n",
      "Epoch 00620 | ep_reward 25.0 | baseline_prev 33.56 -> ema 32.70 | COLLAPSE | extra_upd=2\n",
      "Epoch 00630 | ep_reward 19.0 | baseline_prev 37.79 -> ema 35.91 | COLLAPSE | extra_upd=2\n",
      "Epoch 00640 | ep_reward 83.0 | baseline_prev 32.53 -> ema 37.57 | SPIKE | extra_upd=2\n",
      "Epoch 00650 | ep_reward 29.0 | baseline_prev 40.07 -> ema 38.96 | COLLAPSE | extra_upd=2\n",
      "Epoch 00660 | ep_reward 27.0 | baseline_prev 42.01 -> ema 40.51 | COLLAPSE | extra_upd=2\n",
      "Epoch 00670 | ep_reward 23.0 | baseline_prev 37.71 -> ema 36.24 | COLLAPSE | extra_upd=2\n",
      "Epoch 00680 | ep_reward 76.0 | baseline_prev 38.84 -> ema 42.55 | SPIKE | extra_upd=2\n",
      "Epoch 00690 | ep_reward 33.0 | baseline_prev 57.64 -> ema 55.17 | COLLAPSE | extra_upd=2\n",
      "Epoch 00700 | ep_reward 13.0 | baseline_prev 49.75 -> ema 46.08 | COLLAPSE | extra_upd=2\n",
      "Epoch 00710 | ep_reward 54.0 | baseline_prev 52.76 -> ema 52.88 | SPIKE | extra_upd=2\n",
      "Epoch 00720 | ep_reward 15.0 | baseline_prev 39.81 -> ema 37.33 | COLLAPSE | extra_upd=2\n",
      "Epoch 00730 | ep_reward 23.0 | baseline_prev 38.12 -> ema 36.61 | COLLAPSE | extra_upd=2\n",
      "Epoch 00740 | ep_reward 25.0 | baseline_prev 43.24 -> ema 41.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 00750 | ep_reward 19.0 | baseline_prev 44.00 -> ema 41.50 | COLLAPSE | extra_upd=2\n",
      "Epoch 00760 | ep_reward 67.0 | baseline_prev 38.47 -> ema 41.33 | SPIKE | extra_upd=2\n",
      "Epoch 00770 | ep_reward 33.0 | baseline_prev 40.70 -> ema 39.93 | COLLAPSE | extra_upd=2\n",
      "Epoch 00780 | ep_reward 28.0 | baseline_prev 34.47 -> ema 33.83 | COLLAPSE | extra_upd=2\n",
      "Epoch 00790 | ep_reward 13.0 | baseline_prev 39.58 -> ema 36.93 | COLLAPSE | extra_upd=2\n",
      "Epoch 00800 | ep_reward 39.0 | baseline_prev 53.33 -> ema 51.89 | COLLAPSE | extra_upd=2\n",
      "Epoch 00810 | ep_reward 93.0 | baseline_prev 46.20 -> ema 50.88 | SPIKE | extra_upd=2\n",
      "Epoch 00820 | ep_reward 67.0 | baseline_prev 54.88 -> ema 56.09 | SPIKE | extra_upd=2\n",
      "Epoch 00830 | ep_reward 70.0 | baseline_prev 62.90 -> ema 63.61 | SPIKE | extra_upd=2\n",
      "Epoch 00840 | ep_reward 43.0 | baseline_prev 55.20 -> ema 53.98 | COLLAPSE | extra_upd=2\n",
      "Epoch 00850 | ep_reward 73.0 | baseline_prev 54.47 -> ema 56.32 | SPIKE | extra_upd=2\n",
      "Epoch 00860 | ep_reward 50.0 | baseline_prev 55.25 -> ema 54.72 | COLLAPSE | extra_upd=2\n",
      "Epoch 00870 | ep_reward 58.0 | baseline_prev 49.12 -> ema 50.01 | SPIKE | extra_upd=2\n",
      "Epoch 00880 | ep_reward 131.0 | baseline_prev 62.87 -> ema 69.69 | SPIKE | extra_upd=2\n",
      "Epoch 00890 | ep_reward 17.0 | baseline_prev 47.98 -> ema 44.88 | COLLAPSE | extra_upd=2\n",
      "Epoch 00900 | ep_reward 20.0 | baseline_prev 50.41 -> ema 47.37 | COLLAPSE | extra_upd=2\n",
      "Epoch 00910 | ep_reward 121.0 | baseline_prev 54.43 -> ema 61.09 | SPIKE | extra_upd=2\n",
      "Epoch 00920 | ep_reward 39.0 | baseline_prev 58.21 -> ema 56.29 | COLLAPSE | extra_upd=2\n",
      "Epoch 00930 | ep_reward 81.0 | baseline_prev 45.33 -> ema 48.89 | SPIKE | extra_upd=2\n",
      "Epoch 00940 | ep_reward 133.0 | baseline_prev 50.08 -> ema 58.37 | SPIKE | extra_upd=2\n",
      "Epoch 00950 | ep_reward 41.0 | baseline_prev 57.42 -> ema 55.78 | COLLAPSE | extra_upd=2\n",
      "Epoch 00960 | ep_reward 19.0 | baseline_prev 60.60 -> ema 56.44 | COLLAPSE | extra_upd=2\n",
      "Epoch 00970 | ep_reward 176.0 | baseline_prev 56.24 -> ema 68.22 | SPIKE | extra_upd=2\n",
      "Epoch 00980 | ep_reward 69.0 | baseline_prev 62.51 -> ema 63.16 | SPIKE | extra_upd=2\n",
      "Epoch 00990 | ep_reward 24.0 | baseline_prev 58.26 -> ema 54.83 | COLLAPSE | extra_upd=2\n",
      "Epoch 01000 | ep_reward 78.0 | baseline_prev 54.33 -> ema 56.70 | SPIKE | extra_upd=2\n",
      "Epoch 01010 | ep_reward 72.0 | baseline_prev 57.07 -> ema 58.57 | SPIKE | extra_upd=2\n",
      "Epoch 01020 | ep_reward 18.0 | baseline_prev 95.31 -> ema 87.58 | COLLAPSE | extra_upd=2\n",
      "Epoch 01030 | ep_reward 99.0 | baseline_prev 66.08 -> ema 69.37 | SPIKE | extra_upd=2\n",
      "Epoch 01040 | ep_reward 31.0 | baseline_prev 67.01 -> ema 63.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 01050 | ep_reward 154.0 | baseline_prev 62.72 -> ema 71.85 | SPIKE | extra_upd=2\n",
      "Epoch 01060 | ep_reward 143.0 | baseline_prev 60.80 -> ema 69.02 | SPIKE | extra_upd=2\n",
      "Epoch 01070 | ep_reward 17.0 | baseline_prev 94.57 -> ema 86.82 | COLLAPSE | extra_upd=2\n",
      "Epoch 01080 | ep_reward 158.0 | baseline_prev 100.02 -> ema 105.81 | SPIKE | extra_upd=2\n",
      "Epoch 01090 | ep_reward 99.0 | baseline_prev 94.74 -> ema 95.17 | SPIKE | extra_upd=2\n",
      "Epoch 01100 | ep_reward 54.0 | baseline_prev 113.62 -> ema 107.66 | COLLAPSE | extra_upd=2\n",
      "Epoch 01110 | ep_reward 23.0 | baseline_prev 72.84 -> ema 67.85 | COLLAPSE | extra_upd=2\n",
      "Epoch 01120 | ep_reward 56.0 | baseline_prev 55.94 -> ema 55.94 | SPIKE | extra_upd=2\n",
      "Epoch 01130 | ep_reward 18.0 | baseline_prev 83.41 -> ema 76.86 | COLLAPSE | extra_upd=2\n",
      "Epoch 01140 | ep_reward 11.0 | baseline_prev 89.00 -> ema 81.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 01150 | ep_reward 18.0 | baseline_prev 87.72 -> ema 80.75 | COLLAPSE | extra_upd=2\n",
      "Epoch 01160 | ep_reward 116.0 | baseline_prev 65.47 -> ema 70.52 | SPIKE | extra_upd=2\n",
      "Epoch 01170 | ep_reward 168.0 | baseline_prev 79.59 -> ema 88.43 | SPIKE | extra_upd=2\n",
      "Epoch 01180 | ep_reward 32.0 | baseline_prev 79.13 -> ema 74.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 01190 | ep_reward 171.0 | baseline_prev 92.37 -> ema 100.23 | SPIKE | extra_upd=2\n",
      "Epoch 01200 | ep_reward 56.0 | baseline_prev 82.14 -> ema 79.52 | COLLAPSE | extra_upd=2\n",
      "Epoch 01210 | ep_reward 29.0 | baseline_prev 78.75 -> ema 73.78 | COLLAPSE | extra_upd=2\n",
      "Epoch 01220 | ep_reward 89.0 | baseline_prev 79.32 -> ema 80.29 | SPIKE | extra_upd=2\n",
      "Epoch 01230 | ep_reward 11.0 | baseline_prev 70.22 -> ema 64.30 | COLLAPSE | extra_upd=2\n",
      "Epoch 01240 | ep_reward 16.0 | baseline_prev 84.66 -> ema 77.80 | COLLAPSE | extra_upd=2\n",
      "Epoch 01250 | ep_reward 63.0 | baseline_prev 79.27 -> ema 77.65 | COLLAPSE | extra_upd=2\n",
      "Epoch 01260 | ep_reward 94.0 | baseline_prev 112.97 -> ema 111.07 | COLLAPSE | extra_upd=2\n",
      "Epoch 01270 | ep_reward 14.0 | baseline_prev 126.45 -> ema 115.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 01280 | ep_reward 25.0 | baseline_prev 112.15 -> ema 103.44 | COLLAPSE | extra_upd=2\n",
      "Epoch 01290 | ep_reward 91.0 | baseline_prev 136.83 -> ema 132.25 | COLLAPSE | extra_upd=2\n",
      "Epoch 01300 | ep_reward 105.0 | baseline_prev 117.36 -> ema 116.13 | COLLAPSE | extra_upd=2\n",
      "Epoch 01310 | ep_reward 304.0 | baseline_prev 148.51 -> ema 164.05 | SPIKE | extra_upd=2\n",
      "Epoch 01320 | ep_reward 30.0 | baseline_prev 142.53 -> ema 131.27 | COLLAPSE | extra_upd=2\n",
      "Epoch 01330 | ep_reward 82.0 | baseline_prev 161.09 -> ema 153.18 | COLLAPSE | extra_upd=2\n",
      "Epoch 01340 | ep_reward 39.0 | baseline_prev 165.98 -> ema 153.28 | COLLAPSE | extra_upd=2\n",
      "Epoch 01350 | ep_reward 13.0 | baseline_prev 179.23 -> ema 162.60 | COLLAPSE | extra_upd=2\n",
      "Epoch 01360 | ep_reward 18.0 | baseline_prev 199.04 -> ema 180.94 | COLLAPSE | extra_upd=2\n",
      "Epoch 01370 | ep_reward 35.0 | baseline_prev 173.65 -> ema 159.79 | COLLAPSE | extra_upd=2\n",
      "Epoch 01380 | ep_reward 120.0 | baseline_prev 181.88 -> ema 175.69 | COLLAPSE | extra_upd=2\n",
      "Epoch 01390 | ep_reward 346.0 | baseline_prev 154.98 -> ema 174.08 | SPIKE | extra_upd=2\n",
      "Epoch 01400 | ep_reward 38.0 | baseline_prev 207.52 -> ema 190.57 | COLLAPSE | extra_upd=2\n",
      "Epoch 01410 | ep_reward 162.0 | baseline_prev 182.98 -> ema 180.88 | COLLAPSE | extra_upd=2\n",
      "Epoch 01420 | ep_reward 96.0 | baseline_prev 217.74 -> ema 205.57 | COLLAPSE | extra_upd=2\n",
      "Epoch 01430 | ep_reward 444.0 | baseline_prev 283.31 -> ema 299.38 | SPIKE | extra_upd=2\n",
      "Epoch 01440 | ep_reward 339.0 | baseline_prev 183.70 -> ema 199.23 | SPIKE | extra_upd=2\n",
      "Epoch 01450 | ep_reward 213.0 | baseline_prev 220.14 -> ema 219.43 | COLLAPSE | extra_upd=2\n",
      "Epoch 01460 | ep_reward 140.0 | baseline_prev 203.79 -> ema 197.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 01470 | ep_reward 42.0 | baseline_prev 195.08 -> ema 179.77 | COLLAPSE | extra_upd=2\n",
      "Epoch 01480 | ep_reward 51.0 | baseline_prev 213.78 -> ema 197.50 | COLLAPSE | extra_upd=2\n",
      "Epoch 01490 | ep_reward 72.0 | baseline_prev 180.53 -> ema 169.68 | COLLAPSE | extra_upd=2\n",
      "Epoch 01500 | ep_reward 139.0 | baseline_prev 157.34 -> ema 155.51 | COLLAPSE | extra_upd=2\n",
      "Epoch 01510 | ep_reward 124.0 | baseline_prev 195.32 -> ema 188.19 | COLLAPSE | extra_upd=2\n",
      "Epoch 01520 | ep_reward 148.0 | baseline_prev 214.04 -> ema 207.44 | COLLAPSE | extra_upd=2\n",
      "Epoch 01530 | ep_reward 294.0 | baseline_prev 228.08 -> ema 234.67 | SPIKE | extra_upd=2\n",
      "Epoch 01540 | ep_reward 279.0 | baseline_prev 242.49 -> ema 246.14 | SPIKE | extra_upd=2\n",
      "Epoch 01550 | ep_reward 258.0 | baseline_prev 228.01 -> ema 231.01 | SPIKE | extra_upd=2\n",
      "Epoch 01560 | ep_reward 312.0 | baseline_prev 253.58 -> ema 259.42 | SPIKE | extra_upd=2\n",
      "Epoch 01570 | ep_reward 170.0 | baseline_prev 203.85 -> ema 200.47 | COLLAPSE | extra_upd=2\n",
      "Epoch 01580 | ep_reward 436.0 | baseline_prev 239.61 -> ema 259.25 | SPIKE | extra_upd=2\n",
      "Epoch 01590 | ep_reward 279.0 | baseline_prev 233.90 -> ema 238.41 | SPIKE | extra_upd=2\n",
      "Epoch 01600 | ep_reward 500.0 | baseline_prev 262.57 -> ema 286.32 | SPIKE | extra_upd=2\n",
      "Epoch 01610 | ep_reward 287.0 | baseline_prev 231.73 -> ema 237.26 | SPIKE | extra_upd=2\n",
      "Epoch 01620 | ep_reward 54.0 | baseline_prev 213.70 -> ema 197.73 | COLLAPSE | extra_upd=2\n",
      "Epoch 01630 | ep_reward 209.0 | baseline_prev 287.66 -> ema 279.79 | COLLAPSE | extra_upd=2\n",
      "Epoch 01640 | ep_reward 125.0 | baseline_prev 287.03 -> ema 270.82 | COLLAPSE | extra_upd=2\n",
      "Epoch 01650 | ep_reward 301.0 | baseline_prev 282.61 -> ema 284.45 | SPIKE | extra_upd=2\n",
      "Epoch 01660 | ep_reward 361.0 | baseline_prev 216.51 -> ema 230.96 | SPIKE | extra_upd=2\n",
      "Epoch 01670 | ep_reward 342.0 | baseline_prev 235.44 -> ema 246.10 | SPIKE | extra_upd=2\n",
      "Epoch 01680 | ep_reward 117.0 | baseline_prev 244.47 -> ema 231.72 | COLLAPSE | extra_upd=2\n",
      "Epoch 01690 | ep_reward 48.0 | baseline_prev 279.76 -> ema 256.58 | COLLAPSE | extra_upd=2\n",
      "Epoch 01700 | ep_reward 206.0 | baseline_prev 217.24 -> ema 216.11 | COLLAPSE | extra_upd=2\n",
      "Epoch 01710 | ep_reward 74.0 | baseline_prev 236.00 -> ema 219.80 | COLLAPSE | extra_upd=2\n",
      "Epoch 01720 | ep_reward 293.0 | baseline_prev 212.92 -> ema 220.92 | SPIKE | extra_upd=2\n",
      "Epoch 01730 | ep_reward 275.0 | baseline_prev 300.85 -> ema 298.27 | COLLAPSE | extra_upd=2\n",
      "Epoch 01740 | ep_reward 125.0 | baseline_prev 262.91 -> ema 249.12 | COLLAPSE | extra_upd=2\n",
      "Epoch 01750 | ep_reward 407.0 | baseline_prev 246.56 -> ema 262.60 | SPIKE | extra_upd=2\n",
      "Epoch 01760 | ep_reward 155.0 | baseline_prev 262.10 -> ema 251.39 | COLLAPSE | extra_upd=2\n",
      "Epoch 01770 | ep_reward 73.0 | baseline_prev 234.73 -> ema 218.56 | COLLAPSE | extra_upd=2\n",
      "Epoch 01780 | ep_reward 111.0 | baseline_prev 267.75 -> ema 252.08 | COLLAPSE | extra_upd=2\n",
      "Epoch 01790 | ep_reward 198.0 | baseline_prev 218.94 -> ema 216.84 | COLLAPSE | extra_upd=2\n",
      "Epoch 01800 | ep_reward 267.0 | baseline_prev 193.18 -> ema 200.56 | SPIKE | extra_upd=2\n",
      "Epoch 01810 | ep_reward 66.0 | baseline_prev 245.50 -> ema 227.55 | COLLAPSE | extra_upd=2\n",
      "Epoch 01820 | ep_reward 81.0 | baseline_prev 191.89 -> ema 180.80 | COLLAPSE | extra_upd=2\n",
      "Epoch 01830 | ep_reward 274.0 | baseline_prev 232.52 -> ema 236.66 | SPIKE | extra_upd=2\n",
      "Epoch 01840 | ep_reward 339.0 | baseline_prev 207.25 -> ema 220.42 | SPIKE | extra_upd=2\n",
      "Epoch 01850 | ep_reward 347.0 | baseline_prev 300.44 -> ema 305.09 | SPIKE | extra_upd=2\n",
      "Epoch 01860 | ep_reward 76.0 | baseline_prev 272.46 -> ema 252.81 | COLLAPSE | extra_upd=2\n",
      "Epoch 01870 | ep_reward 64.0 | baseline_prev 213.88 -> ema 198.89 | COLLAPSE | extra_upd=2\n",
      "Epoch 01880 | ep_reward 349.0 | baseline_prev 223.09 -> ema 235.68 | SPIKE | extra_upd=2\n",
      "Epoch 01890 | ep_reward 358.0 | baseline_prev 325.75 -> ema 328.98 | SPIKE | extra_upd=2\n",
      "Epoch 01900 | ep_reward 173.0 | baseline_prev 302.90 -> ema 289.91 | COLLAPSE | extra_upd=2\n",
      "Epoch 01910 | ep_reward 500.0 | baseline_prev 289.82 -> ema 310.84 | SPIKE | extra_upd=2\n",
      "Epoch 01920 | ep_reward 421.0 | baseline_prev 213.15 -> ema 233.94 | SPIKE | extra_upd=2\n",
      "Epoch 01930 | ep_reward 139.0 | baseline_prev 274.38 -> ema 260.84 | COLLAPSE | extra_upd=2\n",
      "Epoch 01940 | ep_reward 424.0 | baseline_prev 270.03 -> ema 285.43 | SPIKE | extra_upd=2\n",
      "Epoch 01950 | ep_reward 500.0 | baseline_prev 275.87 -> ema 298.28 | SPIKE | extra_upd=2\n",
      "Epoch 01960 | ep_reward 253.0 | baseline_prev 281.25 -> ema 278.43 | COLLAPSE | extra_upd=2\n",
      "Epoch 01970 | ep_reward 269.0 | baseline_prev 212.61 -> ema 218.25 | SPIKE | extra_upd=2\n",
      "Epoch 01980 | ep_reward 500.0 | baseline_prev 306.37 -> ema 325.73 | SPIKE | extra_upd=2\n",
      "Epoch 01990 | ep_reward 323.0 | baseline_prev 295.48 -> ema 298.23 | SPIKE | extra_upd=2\n",
      "Epoch 02000 | ep_reward 136.0 | baseline_prev 258.11 -> ema 245.90 | COLLAPSE | extra_upd=2\n",
      "Epoch 02010 | ep_reward 185.0 | baseline_prev 255.17 -> ema 248.16 | COLLAPSE | extra_upd=2\n",
      "Epoch 02020 | ep_reward 290.0 | baseline_prev 232.64 -> ema 238.38 | SPIKE | extra_upd=2\n",
      "Epoch 02030 | ep_reward 15.0 | baseline_prev 233.00 -> ema 211.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 02040 | ep_reward 438.0 | baseline_prev 256.76 -> ema 274.88 | SPIKE | extra_upd=2\n",
      "Epoch 02050 | ep_reward 458.0 | baseline_prev 309.30 -> ema 324.17 | SPIKE | extra_upd=2\n",
      "Epoch 02060 | ep_reward 113.0 | baseline_prev 281.64 -> ema 264.78 | COLLAPSE | extra_upd=2\n",
      "Epoch 02070 | ep_reward 415.0 | baseline_prev 286.04 -> ema 298.93 | SPIKE | extra_upd=2\n",
      "Epoch 02080 | ep_reward 302.0 | baseline_prev 309.86 -> ema 309.07 | COLLAPSE | extra_upd=2\n",
      "Epoch 02090 | ep_reward 241.0 | baseline_prev 322.32 -> ema 314.19 | COLLAPSE | extra_upd=2\n",
      "Epoch 02100 | ep_reward 260.0 | baseline_prev 240.92 -> ema 242.83 | SPIKE | extra_upd=2\n",
      "Epoch 02110 | ep_reward 294.0 | baseline_prev 255.98 -> ema 259.78 | SPIKE | extra_upd=2\n",
      "Epoch 02120 | ep_reward 325.0 | baseline_prev 242.87 -> ema 251.08 | SPIKE | extra_upd=2\n",
      "Epoch 02130 | ep_reward 379.0 | baseline_prev 229.21 -> ema 244.19 | SPIKE | extra_upd=2\n",
      "Epoch 02140 | ep_reward 334.0 | baseline_prev 260.86 -> ema 268.17 | SPIKE | extra_upd=2\n",
      "Epoch 02150 | ep_reward 72.0 | baseline_prev 276.73 -> ema 256.26 | COLLAPSE | extra_upd=2\n",
      "Epoch 02160 | ep_reward 438.0 | baseline_prev 277.25 -> ema 293.33 | SPIKE | extra_upd=2\n",
      "Epoch 02170 | ep_reward 500.0 | baseline_prev 265.97 -> ema 289.38 | SPIKE | extra_upd=2\n",
      "Epoch 02180 | ep_reward 500.0 | baseline_prev 294.30 -> ema 314.87 | SPIKE | extra_upd=2\n",
      "Epoch 02190 | ep_reward 500.0 | baseline_prev 305.79 -> ema 325.21 | SPIKE | extra_upd=2\n",
      "Epoch 02200 | ep_reward 314.0 | baseline_prev 324.06 -> ema 323.05 | COLLAPSE | extra_upd=2\n",
      "Epoch 02210 | ep_reward 500.0 | baseline_prev 289.73 -> ema 310.75 | SPIKE | extra_upd=2\n",
      "Epoch 02220 | ep_reward 66.0 | baseline_prev 319.52 -> ema 294.17 | COLLAPSE | extra_upd=2\n",
      "Epoch 02230 | ep_reward 182.0 | baseline_prev 266.92 -> ema 258.43 | COLLAPSE | extra_upd=2\n",
      "Epoch 02240 | ep_reward 149.0 | baseline_prev 304.20 -> ema 288.68 | COLLAPSE | extra_upd=2\n",
      "Epoch 02250 | ep_reward 299.0 | baseline_prev 302.02 -> ema 301.72 | COLLAPSE | extra_upd=2\n",
      "Epoch 02260 | ep_reward 200.0 | baseline_prev 314.97 -> ema 303.48 | COLLAPSE | extra_upd=2\n",
      "Epoch 02270 | ep_reward 314.0 | baseline_prev 263.80 -> ema 268.82 | SPIKE | extra_upd=2\n",
      "Epoch 02280 | ep_reward 500.0 | baseline_prev 300.13 -> ema 320.12 | SPIKE | extra_upd=2\n",
      "Epoch 02290 | ep_reward 237.0 | baseline_prev 309.60 -> ema 302.34 | COLLAPSE | extra_upd=2\n",
      "Epoch 02300 | ep_reward 345.0 | baseline_prev 254.32 -> ema 263.39 | SPIKE | extra_upd=2\n",
      "Epoch 02310 | ep_reward 338.0 | baseline_prev 317.72 -> ema 319.74 | SPIKE | extra_upd=2\n",
      "Epoch 02320 | ep_reward 112.0 | baseline_prev 299.21 -> ema 280.49 | COLLAPSE | extra_upd=2\n",
      "Epoch 02330 | ep_reward 404.0 | baseline_prev 239.61 -> ema 256.05 | SPIKE | extra_upd=2\n",
      "Epoch 02340 | ep_reward 71.0 | baseline_prev 247.56 -> ema 229.91 | COLLAPSE | extra_upd=2\n",
      "Epoch 02350 | ep_reward 272.0 | baseline_prev 309.13 -> ema 305.42 | COLLAPSE | extra_upd=2\n",
      "Epoch 02360 | ep_reward 259.0 | baseline_prev 310.96 -> ema 305.76 | COLLAPSE | extra_upd=2\n",
      "Epoch 02370 | ep_reward 245.0 | baseline_prev 360.60 -> ema 349.04 | COLLAPSE | extra_upd=2\n",
      "Epoch 02380 | ep_reward 13.0 | baseline_prev 298.87 -> ema 270.28 | COLLAPSE | extra_upd=2\n",
      "Epoch 02390 | ep_reward 500.0 | baseline_prev 236.95 -> ema 263.25 | SPIKE | extra_upd=2\n",
      "Epoch 02400 | ep_reward 240.0 | baseline_prev 326.98 -> ema 318.28 | COLLAPSE | extra_upd=2\n",
      "Epoch 02410 | ep_reward 205.0 | baseline_prev 276.08 -> ema 268.97 | COLLAPSE | extra_upd=2\n",
      "Epoch 02420 | ep_reward 500.0 | baseline_prev 292.64 -> ema 313.37 | SPIKE | extra_upd=2\n",
      "Epoch 02430 | ep_reward 99.0 | baseline_prev 355.14 -> ema 329.53 | COLLAPSE | extra_upd=2\n",
      "Epoch 02440 | ep_reward 31.0 | baseline_prev 224.58 -> ema 205.23 | COLLAPSE | extra_upd=2\n",
      "Epoch 02450 | ep_reward 500.0 | baseline_prev 243.42 -> ema 269.07 | SPIKE | extra_upd=2\n",
      "Epoch 02460 | ep_reward 113.0 | baseline_prev 343.81 -> ema 320.73 | COLLAPSE | extra_upd=2\n",
      "Epoch 02470 | ep_reward 500.0 | baseline_prev 316.34 -> ema 334.71 | SPIKE | extra_upd=2\n",
      "Epoch 02480 | ep_reward 308.0 | baseline_prev 284.75 -> ema 287.07 | SPIKE | extra_upd=2\n",
      "Epoch 02490 | ep_reward 135.0 | baseline_prev 226.37 -> ema 217.23 | COLLAPSE | extra_upd=2\n",
      "Epoch 02500 | ep_reward 246.0 | baseline_prev 301.24 -> ema 295.71 | COLLAPSE | extra_upd=2\n",
      "Epoch 02510 | ep_reward 199.0 | baseline_prev 297.50 -> ema 287.65 | COLLAPSE | extra_upd=2\n",
      "Epoch 02520 | ep_reward 95.0 | baseline_prev 277.70 -> ema 259.43 | COLLAPSE | extra_upd=2\n",
      "Epoch 02530 | ep_reward 158.0 | baseline_prev 200.49 -> ema 196.24 | COLLAPSE | extra_upd=2\n",
      "Epoch 02540 | ep_reward 80.0 | baseline_prev 201.94 -> ema 189.75 | COLLAPSE | extra_upd=2\n",
      "Epoch 02550 | ep_reward 464.0 | baseline_prev 278.52 -> ema 297.06 | SPIKE | extra_upd=2\n",
      "Epoch 02560 | ep_reward 111.0 | baseline_prev 236.85 -> ema 224.27 | COLLAPSE | extra_upd=2\n",
      "Epoch 02570 | ep_reward 202.0 | baseline_prev 219.22 -> ema 217.50 | COLLAPSE | extra_upd=2\n",
      "Epoch 02580 | ep_reward 404.0 | baseline_prev 327.19 -> ema 334.87 | SPIKE | extra_upd=2\n",
      "Epoch 02590 | ep_reward 500.0 | baseline_prev 270.94 -> ema 293.85 | SPIKE | extra_upd=2\n",
      "Epoch 02600 | ep_reward 56.0 | baseline_prev 297.45 -> ema 273.30 | COLLAPSE | extra_upd=2\n",
      "Epoch 02610 | ep_reward 322.0 | baseline_prev 226.47 -> ema 236.02 | SPIKE | extra_upd=2\n",
      "Epoch 02620 | ep_reward 488.0 | baseline_prev 289.87 -> ema 309.68 | SPIKE | extra_upd=2\n",
      "Epoch 02630 | ep_reward 84.0 | baseline_prev 251.28 -> ema 234.55 | COLLAPSE | extra_upd=2\n",
      "Epoch 02640 | ep_reward 350.0 | baseline_prev 324.42 -> ema 326.98 | SPIKE | extra_upd=2\n",
      "Epoch 02650 | ep_reward 108.0 | baseline_prev 306.13 -> ema 286.32 | COLLAPSE | extra_upd=2\n",
      "Epoch 02660 | ep_reward 16.0 | baseline_prev 274.06 -> ema 248.26 | COLLAPSE | extra_upd=2\n",
      "Epoch 02670 | ep_reward 265.0 | baseline_prev 269.29 -> ema 268.86 | COLLAPSE | extra_upd=2\n",
      "Epoch 02680 | ep_reward 500.0 | baseline_prev 358.61 -> ema 372.75 | SPIKE | extra_upd=2\n",
      "Epoch 02690 | ep_reward 234.0 | baseline_prev 343.72 -> ema 332.75 | COLLAPSE | extra_upd=2\n",
      "Epoch 02700 | ep_reward 220.0 | baseline_prev 283.56 -> ema 277.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 02710 | ep_reward 197.0 | baseline_prev 317.76 -> ema 305.68 | COLLAPSE | extra_upd=2\n",
      "Epoch 02720 | ep_reward 359.0 | baseline_prev 298.71 -> ema 304.74 | SPIKE | extra_upd=2\n",
      "Epoch 02730 | ep_reward 500.0 | baseline_prev 328.44 -> ema 345.60 | SPIKE | extra_upd=2\n",
      "Epoch 02740 | ep_reward 165.0 | baseline_prev 281.90 -> ema 270.21 | COLLAPSE | extra_upd=2\n",
      "Epoch 02750 | ep_reward 190.0 | baseline_prev 319.64 -> ema 306.67 | COLLAPSE | extra_upd=2\n",
      "Epoch 02760 | ep_reward 458.0 | baseline_prev 245.81 -> ema 267.03 | SPIKE | extra_upd=2\n",
      "Epoch 02770 | ep_reward 369.0 | baseline_prev 247.55 -> ema 259.70 | SPIKE | extra_upd=2\n",
      "Epoch 02780 | ep_reward 500.0 | baseline_prev 290.43 -> ema 311.38 | SPIKE | extra_upd=2\n",
      "Epoch 02790 | ep_reward 376.0 | baseline_prev 286.74 -> ema 295.66 | SPIKE | extra_upd=2\n",
      "Epoch 02800 | ep_reward 170.0 | baseline_prev 338.16 -> ema 321.34 | COLLAPSE | extra_upd=2\n",
      "Epoch 02810 | ep_reward 465.0 | baseline_prev 297.22 -> ema 314.00 | SPIKE | extra_upd=2\n",
      "Epoch 02820 | ep_reward 40.0 | baseline_prev 291.83 -> ema 266.64 | COLLAPSE | extra_upd=2\n",
      "Epoch 02830 | ep_reward 500.0 | baseline_prev 253.16 -> ema 277.85 | SPIKE | extra_upd=2\n",
      "Epoch 02840 | ep_reward 303.0 | baseline_prev 249.44 -> ema 254.80 | SPIKE | extra_upd=2\n",
      "Epoch 02850 | ep_reward 33.0 | baseline_prev 326.63 -> ema 297.27 | COLLAPSE | extra_upd=2\n",
      "Epoch 02860 | ep_reward 169.0 | baseline_prev 344.56 -> ema 327.00 | COLLAPSE | extra_upd=2\n",
      "Epoch 02870 | ep_reward 448.0 | baseline_prev 227.17 -> ema 249.25 | SPIKE | extra_upd=2\n",
      "Epoch 02880 | ep_reward 131.0 | baseline_prev 262.57 -> ema 249.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 02890 | ep_reward 500.0 | baseline_prev 260.07 -> ema 284.07 | SPIKE | extra_upd=2\n",
      "Epoch 02900 | ep_reward 313.0 | baseline_prev 254.63 -> ema 260.46 | SPIKE | extra_upd=2\n",
      "Epoch 02910 | ep_reward 500.0 | baseline_prev 260.38 -> ema 284.34 | SPIKE | extra_upd=2\n",
      "Epoch 02920 | ep_reward 337.0 | baseline_prev 273.71 -> ema 280.04 | SPIKE | extra_upd=2\n",
      "Epoch 02930 | ep_reward 310.0 | baseline_prev 265.93 -> ema 270.33 | SPIKE | extra_upd=2\n",
      "Epoch 02940 | ep_reward 191.0 | baseline_prev 333.15 -> ema 318.93 | COLLAPSE | extra_upd=2\n",
      "Epoch 02950 | ep_reward 500.0 | baseline_prev 317.71 -> ema 335.94 | SPIKE | extra_upd=2\n",
      "Epoch 02960 | ep_reward 301.0 | baseline_prev 378.27 -> ema 370.54 | COLLAPSE | extra_upd=2\n",
      "Epoch 02970 | ep_reward 500.0 | baseline_prev 299.67 -> ema 319.71 | SPIKE | extra_upd=2\n",
      "Epoch 02980 | ep_reward 93.0 | baseline_prev 293.57 -> ema 273.51 | COLLAPSE | extra_upd=2\n",
      "Epoch 02990 | ep_reward 37.0 | baseline_prev 253.44 -> ema 231.80 | COLLAPSE | extra_upd=2\n",
      "Epoch 03000 | ep_reward 349.0 | baseline_prev 253.21 -> ema 262.79 | SPIKE | extra_upd=2\n",
      "Epoch 03010 | ep_reward 500.0 | baseline_prev 283.47 -> ema 305.13 | SPIKE | extra_upd=2\n",
      "Epoch 03020 | ep_reward 338.0 | baseline_prev 337.29 -> ema 337.36 | SPIKE | extra_upd=2\n",
      "Epoch 03030 | ep_reward 500.0 | baseline_prev 312.91 -> ema 331.62 | SPIKE | extra_upd=2\n",
      "Epoch 03040 | ep_reward 180.0 | baseline_prev 377.30 -> ema 357.57 | COLLAPSE | extra_upd=2\n",
      "Epoch 03050 | ep_reward 86.0 | baseline_prev 310.85 -> ema 288.37 | COLLAPSE | extra_upd=2\n",
      "Epoch 03060 | ep_reward 342.0 | baseline_prev 350.40 -> ema 349.56 | COLLAPSE | extra_upd=2\n",
      "Epoch 03070 | ep_reward 500.0 | baseline_prev 318.27 -> ema 336.44 | SPIKE | extra_upd=2\n",
      "Epoch 03080 | ep_reward 500.0 | baseline_prev 322.71 -> ema 340.44 | SPIKE | extra_upd=2\n",
      "Epoch 03090 | ep_reward 73.0 | baseline_prev 326.55 -> ema 301.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 03100 | ep_reward 278.0 | baseline_prev 318.75 -> ema 314.68 | COLLAPSE | extra_upd=2\n",
      "Epoch 03110 | ep_reward 235.0 | baseline_prev 240.41 -> ema 239.87 | COLLAPSE | extra_upd=2\n",
      "Epoch 03120 | ep_reward 337.0 | baseline_prev 311.88 -> ema 314.39 | SPIKE | extra_upd=2\n",
      "Epoch 03130 | ep_reward 310.0 | baseline_prev 269.75 -> ema 273.77 | SPIKE | extra_upd=2\n",
      "Epoch 03140 | ep_reward 500.0 | baseline_prev 296.02 -> ema 316.41 | SPIKE | extra_upd=2\n",
      "Epoch 03150 | ep_reward 402.0 | baseline_prev 322.08 -> ema 330.07 | SPIKE | extra_upd=2\n",
      "Epoch 03160 | ep_reward 500.0 | baseline_prev 304.85 -> ema 324.37 | SPIKE | extra_upd=2\n",
      "Epoch 03170 | ep_reward 354.0 | baseline_prev 308.88 -> ema 313.39 | SPIKE | extra_upd=2\n",
      "Epoch 03180 | ep_reward 500.0 | baseline_prev 272.62 -> ema 295.35 | SPIKE | extra_upd=2\n",
      "Epoch 03190 | ep_reward 500.0 | baseline_prev 281.74 -> ema 303.57 | SPIKE | extra_upd=2\n",
      "Epoch 03200 | ep_reward 443.0 | baseline_prev 326.89 -> ema 338.50 | SPIKE | extra_upd=2\n",
      "Epoch 03210 | ep_reward 500.0 | baseline_prev 343.89 -> ema 359.50 | SPIKE | extra_upd=2\n",
      "Epoch 03220 | ep_reward 41.0 | baseline_prev 378.05 -> ema 344.34 | COLLAPSE | extra_upd=2\n",
      "Epoch 03230 | ep_reward 500.0 | baseline_prev 252.31 -> ema 277.08 | SPIKE | extra_upd=2\n",
      "Epoch 03240 | ep_reward 361.0 | baseline_prev 323.35 -> ema 327.11 | SPIKE | extra_upd=2\n",
      "Epoch 03250 | ep_reward 477.0 | baseline_prev 254.35 -> ema 276.62 | SPIKE | extra_upd=2\n",
      "Epoch 03260 | ep_reward 94.0 | baseline_prev 251.78 -> ema 236.00 | COLLAPSE | extra_upd=2\n",
      "Epoch 03270 | ep_reward 500.0 | baseline_prev 265.98 -> ema 289.38 | SPIKE | extra_upd=2\n",
      "Epoch 03280 | ep_reward 132.0 | baseline_prev 365.53 -> ema 342.17 | COLLAPSE | extra_upd=2\n",
      "Epoch 03290 | ep_reward 342.0 | baseline_prev 305.31 -> ema 308.98 | SPIKE | extra_upd=2\n",
      "Epoch 03300 | ep_reward 500.0 | baseline_prev 320.74 -> ema 338.66 | SPIKE | extra_upd=2\n",
      "Epoch 03310 | ep_reward 128.0 | baseline_prev 301.33 -> ema 284.00 | COLLAPSE | extra_upd=2\n",
      "Epoch 03320 | ep_reward 500.0 | baseline_prev 315.88 -> ema 334.29 | SPIKE | extra_upd=2\n",
      "Epoch 03330 | ep_reward 201.0 | baseline_prev 350.10 -> ema 335.19 | COLLAPSE | extra_upd=2\n",
      "Epoch 03340 | ep_reward 291.0 | baseline_prev 268.64 -> ema 270.88 | SPIKE | extra_upd=2\n",
      "Epoch 03350 | ep_reward 128.0 | baseline_prev 351.74 -> ema 329.37 | COLLAPSE | extra_upd=2\n",
      "Epoch 03360 | ep_reward 216.0 | baseline_prev 370.95 -> ema 355.45 | COLLAPSE | extra_upd=2\n",
      "Epoch 03370 | ep_reward 311.0 | baseline_prev 338.76 -> ema 335.98 | COLLAPSE | extra_upd=2\n",
      "Epoch 03380 | ep_reward 410.0 | baseline_prev 258.13 -> ema 273.32 | SPIKE | extra_upd=2\n",
      "Epoch 03390 | ep_reward 133.0 | baseline_prev 337.27 -> ema 316.84 | COLLAPSE | extra_upd=2\n",
      "Epoch 03400 | ep_reward 500.0 | baseline_prev 305.23 -> ema 324.70 | SPIKE | extra_upd=2\n",
      "Epoch 03410 | ep_reward 182.0 | baseline_prev 314.62 -> ema 301.36 | COLLAPSE | extra_upd=2\n",
      "Epoch 03420 | ep_reward 438.0 | baseline_prev 287.14 -> ema 302.23 | SPIKE | extra_upd=2\n",
      "Epoch 03430 | ep_reward 500.0 | baseline_prev 354.52 -> ema 369.06 | SPIKE | extra_upd=2\n",
      "Epoch 03440 | ep_reward 500.0 | baseline_prev 343.62 -> ema 359.26 | SPIKE | extra_upd=2\n",
      "Epoch 03450 | ep_reward 375.0 | baseline_prev 330.79 -> ema 335.21 | SPIKE | extra_upd=2\n",
      "Epoch 03460 | ep_reward 500.0 | baseline_prev 352.95 -> ema 367.65 | SPIKE | extra_upd=2\n",
      "Epoch 03470 | ep_reward 391.0 | baseline_prev 285.10 -> ema 295.69 | SPIKE | extra_upd=2\n",
      "Epoch 03480 | ep_reward 124.0 | baseline_prev 283.73 -> ema 267.75 | COLLAPSE | extra_upd=2\n",
      "Epoch 03490 | ep_reward 377.0 | baseline_prev 341.07 -> ema 344.66 | SPIKE | extra_upd=2\n",
      "Epoch 03500 | ep_reward 437.0 | baseline_prev 363.99 -> ema 371.29 | SPIKE | extra_upd=2\n",
      "Epoch 03510 | ep_reward 500.0 | baseline_prev 317.45 -> ema 335.71 | SPIKE | extra_upd=2\n",
      "Epoch 03520 | ep_reward 461.0 | baseline_prev 312.58 -> ema 327.42 | SPIKE | extra_upd=2\n",
      "Epoch 03530 | ep_reward 67.0 | baseline_prev 385.83 -> ema 353.94 | COLLAPSE | extra_upd=2\n",
      "Epoch 03540 | ep_reward 365.0 | baseline_prev 342.92 -> ema 345.12 | SPIKE | extra_upd=2\n",
      "Epoch 03550 | ep_reward 335.0 | baseline_prev 392.90 -> ema 387.11 | COLLAPSE | extra_upd=2\n",
      "Epoch 03560 | ep_reward 306.0 | baseline_prev 325.80 -> ema 323.82 | COLLAPSE | extra_upd=2\n",
      "Epoch 03570 | ep_reward 206.0 | baseline_prev 288.17 -> ema 279.96 | COLLAPSE | extra_upd=2\n",
      "Epoch 03580 | ep_reward 57.0 | baseline_prev 282.79 -> ema 260.21 | COLLAPSE | extra_upd=2\n",
      "Epoch 03590 | ep_reward 500.0 | baseline_prev 307.59 -> ema 326.83 | SPIKE | extra_upd=2\n",
      "Epoch 03600 | ep_reward 181.0 | baseline_prev 296.84 -> ema 285.26 | COLLAPSE | extra_upd=2\n",
      "Epoch 03610 | ep_reward 100.0 | baseline_prev 222.99 -> ema 210.69 | COLLAPSE | extra_upd=2\n",
      "Epoch 03620 | ep_reward 201.0 | baseline_prev 276.96 -> ema 269.37 | COLLAPSE | extra_upd=2\n",
      "Epoch 03630 | ep_reward 134.0 | baseline_prev 214.97 -> ema 206.87 | COLLAPSE | extra_upd=2\n",
      "Epoch 03640 | ep_reward 266.0 | baseline_prev 260.22 -> ema 260.80 | SPIKE | extra_upd=2\n",
      "Epoch 03650 | ep_reward 280.0 | baseline_prev 306.25 -> ema 303.62 | COLLAPSE | extra_upd=2\n",
      "Epoch 03660 | ep_reward 181.0 | baseline_prev 355.24 -> ema 337.82 | COLLAPSE | extra_upd=2\n",
      "Epoch 03670 | ep_reward 500.0 | baseline_prev 285.73 -> ema 307.16 | SPIKE | extra_upd=2\n",
      "Epoch 03680 | ep_reward 500.0 | baseline_prev 292.45 -> ema 313.21 | SPIKE | extra_upd=2\n",
      "Epoch 03690 | ep_reward 500.0 | baseline_prev 361.60 -> ema 375.44 | SPIKE | extra_upd=2\n",
      "Epoch 03700 | ep_reward 57.0 | baseline_prev 335.04 -> ema 307.24 | COLLAPSE | extra_upd=2\n",
      "Epoch 03710 | ep_reward 37.0 | baseline_prev 385.21 -> ema 350.39 | COLLAPSE | extra_upd=2\n",
      "Epoch 03720 | ep_reward 500.0 | baseline_prev 346.62 -> ema 361.96 | SPIKE | extra_upd=2\n",
      "Epoch 03730 | ep_reward 500.0 | baseline_prev 330.20 -> ema 347.18 | SPIKE | extra_upd=2\n",
      "Epoch 03740 | ep_reward 212.0 | baseline_prev 319.88 -> ema 309.09 | COLLAPSE | extra_upd=2\n",
      "Epoch 03750 | ep_reward 370.0 | baseline_prev 299.97 -> ema 306.98 | SPIKE | extra_upd=2\n",
      "Epoch 03760 | ep_reward 442.0 | baseline_prev 361.12 -> ema 369.21 | SPIKE | extra_upd=2\n",
      "Epoch 03770 | ep_reward 217.0 | baseline_prev 354.64 -> ema 340.88 | COLLAPSE | extra_upd=2\n",
      "Epoch 03780 | ep_reward 338.0 | baseline_prev 303.76 -> ema 307.18 | SPIKE | extra_upd=2\n",
      "Epoch 03790 | ep_reward 192.0 | baseline_prev 322.51 -> ema 309.46 | COLLAPSE | extra_upd=2\n",
      "Epoch 03800 | ep_reward 85.0 | baseline_prev 334.35 -> ema 309.41 | COLLAPSE | extra_upd=2\n",
      "Epoch 03810 | ep_reward 500.0 | baseline_prev 371.22 -> ema 384.10 | SPIKE | extra_upd=2\n",
      "Epoch 03820 | ep_reward 130.0 | baseline_prev 299.76 -> ema 282.78 | COLLAPSE | extra_upd=2\n",
      "Epoch 03830 | ep_reward 500.0 | baseline_prev 269.54 -> ema 292.59 | SPIKE | extra_upd=2\n",
      "Epoch 03840 | ep_reward 296.0 | baseline_prev 315.54 -> ema 313.58 | COLLAPSE | extra_upd=2\n",
      "Epoch 03850 | ep_reward 340.0 | baseline_prev 264.36 -> ema 271.92 | SPIKE | extra_upd=2\n",
      "Epoch 03860 | ep_reward 276.0 | baseline_prev 285.49 -> ema 284.55 | COLLAPSE | extra_upd=2\n",
      "Epoch 03870 | ep_reward 79.0 | baseline_prev 271.00 -> ema 251.80 | COLLAPSE | extra_upd=2\n",
      "Epoch 03880 | ep_reward 310.0 | baseline_prev 237.76 -> ema 244.98 | SPIKE | extra_upd=2\n",
      "Epoch 03890 | ep_reward 252.0 | baseline_prev 292.54 -> ema 288.48 | COLLAPSE | extra_upd=2\n",
      "Epoch 03900 | ep_reward 242.0 | baseline_prev 305.44 -> ema 299.09 | COLLAPSE | extra_upd=2\n",
      "Epoch 03910 | ep_reward 432.0 | baseline_prev 287.49 -> ema 301.94 | SPIKE | extra_upd=2\n",
      "Epoch 03920 | ep_reward 244.0 | baseline_prev 296.32 -> ema 291.09 | COLLAPSE | extra_upd=2\n",
      "Epoch 03930 | ep_reward 500.0 | baseline_prev 291.18 -> ema 312.06 | SPIKE | extra_upd=2\n",
      "Epoch 03940 | ep_reward 250.0 | baseline_prev 351.93 -> ema 341.74 | COLLAPSE | extra_upd=2\n",
      "Epoch 03950 | ep_reward 440.0 | baseline_prev 282.82 -> ema 298.54 | SPIKE | extra_upd=2\n",
      "Epoch 03960 | ep_reward 125.0 | baseline_prev 234.11 -> ema 223.20 | COLLAPSE | extra_upd=2\n",
      "Epoch 03970 | ep_reward 274.0 | baseline_prev 291.84 -> ema 290.06 | COLLAPSE | extra_upd=2\n",
      "Epoch 03980 | ep_reward 261.0 | baseline_prev 321.10 -> ema 315.09 | COLLAPSE | extra_upd=2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(state\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    183\u001b[0m state_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 184\u001b[0m action, (a_hx, a_cx) \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_hx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_cx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m    187\u001b[0m next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[3], line 82\u001b[0m, in \u001b[0;36mPolicyNetwork.select_action\u001b[1;34m(self, state, hidden)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, hidden):\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 82\u001b[0m         prob, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m         b \u001b[38;5;241m=\u001b[39m Bernoulli(prob)\n\u001b[0;32m     84\u001b[0m         action \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[3], line 75\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[0;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 75\u001b[0m     x, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))  \u001b[38;5;66;03m# (B, T, 1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\82108\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\82108\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\82108\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "# =========================\n",
    "# Tunable hyperparameters\n",
    "# =========================\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Memory / model capacity\n",
    "ACTOR_HIDDEN = 256\n",
    "CRITIC_HIDDEN = 256\n",
    "LSTM_LAYERS = 1\n",
    "LSTM_DROPOUT = 0.1\n",
    "\n",
    "# Optimization stability\n",
    "LR_ACTOR = 2e-4\n",
    "LR_CRITIC = 2e-4\n",
    "CLIP_NORM = 0.5\n",
    "\n",
    "# Exploration / collapse prevention\n",
    "ENT_COEF = 0.01\n",
    "ADV_NORM = True\n",
    "USE_HUBER_VALUE_LOSS = True\n",
    "\n",
    "# =========================\n",
    "# Catch & Climb (NEW)\n",
    "# =========================\n",
    "# baseline is an EMA of episode_reward; collapse/spike is judged vs previous baseline\n",
    "BASELINE_BETA = 0.90          # EMA smoothing; bigger -> slower baseline\n",
    "EPS = 1e-6\n",
    "\n",
    "# collapse/spike intensity normalization\n",
    "NORM_BY = \"abs_baseline\"      # or \"maxlen\" etc. we'll use abs baseline for stability\n",
    "\n",
    "# Anchor (anti-collapse): keep best policy snapshot; on collapse, penalize KL(current || anchor)\n",
    "AUX_KL_COEF = 0.5             # base KL weight\n",
    "AUX_KL_MAX = 5.0              # cap when collapse is huge\n",
    "\n",
    "# Spike (self-imitation): on spike, do extra log-prob maximize on that trajectory\n",
    "SIL_COEF = 0.5                # base imitation weight\n",
    "SIL_MAX = 5.0                 # cap\n",
    "\n",
    "# Extra gradient steps when an event happens (collapse or spike)\n",
    "EXTRA_UPDATES_ON_EVENT = 2     # do additional updates using same trajectory\n",
    "EXTRA_UPDATES_CAP = 6          # safety cap if you later scale dynamically\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device =\", device)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            64, ACTOR_HIDDEN,\n",
    "            num_layers=LSTM_LAYERS,\n",
    "            dropout=(LSTM_DROPOUT if LSTM_LAYERS >= 2 else 0.0),\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc2 = nn.Linear(ACTOR_HIDDEN, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.sigmoid(self.fc2(x))  # (B, T, 1)\n",
    "        return x, hidden\n",
    "\n",
    "    def select_action(self, state, hidden):\n",
    "        with torch.no_grad():\n",
    "            prob, hidden = self.forward(state, hidden)\n",
    "            b = Bernoulli(prob)\n",
    "            action = b.sample()\n",
    "        return int(action.item()), hidden\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.lstm = nn.LSTM(\n",
    "            64, CRITIC_HIDDEN,\n",
    "            num_layers=LSTM_LAYERS,\n",
    "            dropout=(LSTM_DROPOUT if LSTM_LAYERS >= 2 else 0.0),\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc2 = nn.Linear(CRITIC_HIDDEN, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "def obs_to_partial(obs):\n",
    "    return np.array([obs[0], obs[2]], dtype=np.float32)\n",
    "\n",
    "\n",
    "def bernoulli_kl(p, q, eps=1e-6):\n",
    "    \"\"\"\n",
    "    KL(Ber(p) || Ber(q)) = p log(p/q) + (1-p) log((1-p)/(1-q))\n",
    "    p,q: tensors in (0,1)\n",
    "    \"\"\"\n",
    "    p = torch.clamp(p, eps, 1.0 - eps)\n",
    "    q = torch.clamp(q, eps, 1.0 - eps)\n",
    "    return p * torch.log(p / q) + (1.0 - p) * torch.log((1.0 - p) / (1.0 - q))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_policy_probs(policy_net, states_tensor):\n",
    "    a_hx = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=states_tensor.device)\n",
    "    a_cx = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=states_tensor.device)\n",
    "    prob, _ = policy_net(states_tensor, (a_hx, a_cx))  # (1,T,1)\n",
    "    return prob.squeeze(0)  # (T,1)\n",
    "\n",
    "\n",
    "def compute_climb_signals(ep_reward, baseline_prev):\n",
    "    \"\"\"\n",
    "    collapse: ep_reward < baseline_prev\n",
    "    spike   : ep_reward > baseline_prev\n",
    "\n",
    "    intensity is normalized so scale is stable.\n",
    "    \"\"\"\n",
    "    denom = abs(baseline_prev) + 1.0  # +1 to avoid tiny denom when baseline near 0\n",
    "    collapse_raw = max(0.0, baseline_prev - ep_reward)\n",
    "    spike_raw = max(0.0, ep_reward - baseline_prev)\n",
    "\n",
    "    collapse_int = collapse_raw / (denom + EPS)\n",
    "    spike_int = spike_raw / (denom + EPS)\n",
    "\n",
    "    is_collapse = 1.0 if ep_reward < baseline_prev else 0.0\n",
    "    is_spike = 1.0 if ep_reward > baseline_prev else 0.0\n",
    "    return is_collapse, is_spike, collapse_int, spike_int\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    policy = PolicyNetwork().to(device)\n",
    "    value = ValueNetwork().to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(policy.parameters(), lr=LR_ACTOR)\n",
    "    value_optim = torch.optim.Adam(value.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "    writer = SummaryWriter(\"./lstm_logs_catch_climb\")\n",
    "\n",
    "    # Anchor snapshot (best policy so far)\n",
    "    anchor_policy = PolicyNetwork().to(device)\n",
    "    anchor_policy.load_state_dict(policy.state_dict())\n",
    "    anchor_policy.eval()\n",
    "\n",
    "    best_reward = -1e9\n",
    "    baseline = 0.0  # EMA baseline\n",
    "\n",
    "    for epoch in count():\n",
    "        obs, info = env.reset(seed=None)\n",
    "        state = obs_to_partial(obs)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        # actor hidden init\n",
    "        a_hx = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=device)\n",
    "        a_cx = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=device)\n",
    "\n",
    "        rewards, actions, states = [], [], []\n",
    "\n",
    "        for t in range(500):\n",
    "            states.append(state.copy())\n",
    "\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=device).view(1, 1, 2)\n",
    "            action, (a_hx, a_cx) = policy.select_action(state_t, (a_hx, a_cx))\n",
    "            actions.append(action)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_state = obs_to_partial(next_obs)\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            rewards.append(float(reward))\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # -------------------------\n",
    "        # returns\n",
    "        # -------------------------\n",
    "        returns = np.zeros(len(rewards), dtype=np.float32)\n",
    "        R = 0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = GAMMA * R + rewards[i]\n",
    "            returns[i] = R\n",
    "\n",
    "        # normalize returns (stability)\n",
    "        mean, std = returns.mean(), returns.std()\n",
    "        std = std if std > 1e-8 else 1.0\n",
    "        returns_norm = (returns - mean) / std\n",
    "\n",
    "        # tensors\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32, device=device).unsqueeze(0)  # (1,T,2)\n",
    "        actions_tensor = torch.tensor(np.array(actions), dtype=torch.float32, device=device).view(-1, 1)  # (T,1)\n",
    "        returns_tensor = torch.tensor(returns_norm, dtype=torch.float32, device=device).view(-1, 1)       # (T,1)\n",
    "\n",
    "        # -------------------------\n",
    "        # Catch & Climb signals\n",
    "        # -------------------------\n",
    "        baseline_prev = float(baseline)\n",
    "        is_collapse, is_spike, collapse_int, spike_int = compute_climb_signals(\n",
    "            ep_reward=episode_reward,\n",
    "            baseline_prev=baseline_prev\n",
    "        )\n",
    "\n",
    "        # update baseline AFTER computing signal\n",
    "        baseline = BASELINE_BETA * baseline + (1.0 - BASELINE_BETA) * float(episode_reward)\n",
    "\n",
    "        # compute dynamic aux weights\n",
    "        kl_w = min(AUX_KL_MAX, AUX_KL_COEF * (1.0 + 5.0 * collapse_int)) if is_collapse else 0.0\n",
    "        sil_w = min(SIL_MAX,    SIL_COEF    * (1.0 + 5.0 * spike_int))   if is_spike else 0.0\n",
    "\n",
    "        # decide extra updates\n",
    "        extra_updates = 0\n",
    "        if is_collapse or is_spike:\n",
    "            extra_updates = int(min(EXTRA_UPDATES_CAP, EXTRA_UPDATES_ON_EVENT))\n",
    "\n",
    "        # -------------------------\n",
    "        # critic baseline & advantage\n",
    "        # -------------------------\n",
    "        with torch.no_grad():\n",
    "            c_hx = torch.zeros((LSTM_LAYERS, 1, CRITIC_HIDDEN), device=device)\n",
    "            c_cx = torch.zeros((LSTM_LAYERS, 1, CRITIC_HIDDEN), device=device)\n",
    "            v, _ = value(states_tensor, (c_hx, c_cx))\n",
    "            v = v.squeeze(0)  # (T,1)\n",
    "\n",
    "            advantage = returns_tensor - v\n",
    "            if ADV_NORM:\n",
    "                advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "        # -------------------------\n",
    "        # prepare anchor probs once\n",
    "        # -------------------------\n",
    "        with torch.no_grad():\n",
    "            anchor_prob = forward_policy_probs(anchor_policy, states_tensor)  # (T,1)\n",
    "\n",
    "        # -------------------------\n",
    "        # update actor (and extra updates if event)\n",
    "        # -------------------------\n",
    "        def actor_step():\n",
    "            a_hx0 = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=device)\n",
    "            a_cx0 = torch.zeros((LSTM_LAYERS, 1, ACTOR_HIDDEN), device=device)\n",
    "            prob, _ = policy(states_tensor, (a_hx0, a_cx0))\n",
    "            prob = prob.squeeze(0)  # (T,1)\n",
    "\n",
    "            dist = Bernoulli(prob)\n",
    "            log_prob = dist.log_prob(actions_tensor)   # (T,1)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # base REINFORCE\n",
    "            base_loss = -(log_prob * advantage.detach()).mean() - ENT_COEF * entropy\n",
    "\n",
    "            # (1) anti-collapse: anchor KL clamp\n",
    "            # enforce: don't drift away from best-known policy when you start collapsing\n",
    "            kl_loss = 0.0\n",
    "            if kl_w > 0.0:\n",
    "                kl = bernoulli_kl(prob, anchor_prob).mean()\n",
    "                kl_loss = float(kl_w) * kl\n",
    "\n",
    "            # (2) spike = self-imitation: aggressively reinforce the \"lucky\" trajectory\n",
    "            # simplest: extra negative log-likelihood of taken actions (BC) weighted by positive advantage\n",
    "            sil_loss = 0.0\n",
    "            if sil_w > 0.0:\n",
    "                pos_adv = torch.clamp(advantage.detach(), min=0.0)\n",
    "                # if all adv <= 0, it becomes 0 (safe)\n",
    "                sil_loss = float(sil_w) * (-(log_prob * pos_adv).mean())\n",
    "\n",
    "            loss = base_loss + kl_loss + sil_loss\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), CLIP_NORM)\n",
    "            optim.step()\n",
    "            return loss, base_loss, kl_loss, sil_loss, entropy\n",
    "\n",
    "        actor_loss, base_loss, kl_loss, sil_loss, entropy = actor_step()\n",
    "        for _ in range(extra_updates):\n",
    "            actor_loss, base_loss, kl_loss, sil_loss, entropy = actor_step()\n",
    "\n",
    "        # -------------------------\n",
    "        # critic update\n",
    "        # -------------------------\n",
    "        c_hx = torch.zeros((LSTM_LAYERS, 1, CRITIC_HIDDEN), device=device)\n",
    "        c_cx = torch.zeros((LSTM_LAYERS, 1, CRITIC_HIDDEN), device=device)\n",
    "        v_pred, _ = value(states_tensor, (c_hx, c_cx))\n",
    "        v_pred = v_pred.squeeze(0)\n",
    "\n",
    "        if USE_HUBER_VALUE_LOSS:\n",
    "            value_loss = F.smooth_l1_loss(v_pred, returns_tensor)\n",
    "        else:\n",
    "            value_loss = F.mse_loss(v_pred, returns_tensor)\n",
    "\n",
    "        value_optim.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(value.parameters(), CLIP_NORM)\n",
    "        value_optim.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # Anchor update (best policy snapshot)\n",
    "        # -------------------------\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = float(episode_reward)\n",
    "            anchor_policy.load_state_dict(policy.state_dict())\n",
    "            anchor_policy.eval()\n",
    "\n",
    "        # -------------------------\n",
    "        # Logging\n",
    "        # -------------------------\n",
    "        writer.add_scalar(\"episode_reward\", episode_reward, epoch)\n",
    "        writer.add_scalar(\"baseline/ema\", baseline, epoch)\n",
    "        writer.add_scalar(\"baseline/prev\", baseline_prev, epoch)\n",
    "        writer.add_scalar(\"catch/is_collapse\", is_collapse, epoch)\n",
    "        writer.add_scalar(\"catch/is_spike\", is_spike, epoch)\n",
    "        writer.add_scalar(\"catch/collapse_int\", collapse_int, epoch)\n",
    "        writer.add_scalar(\"catch/spike_int\", spike_int, epoch)\n",
    "        writer.add_scalar(\"catch/kl_weight\", kl_w, epoch)\n",
    "        writer.add_scalar(\"catch/sil_weight\", sil_w, epoch)\n",
    "        writer.add_scalar(\"catch/extra_updates\", extra_updates, epoch)\n",
    "\n",
    "        writer.add_scalar(\"loss/actor_total\", float(actor_loss.item()), epoch)\n",
    "        writer.add_scalar(\"loss/actor_base\", float(base_loss.item()), epoch)\n",
    "        writer.add_scalar(\"loss/actor_kl\", float(kl_loss) if isinstance(kl_loss, float) else float(kl_loss.item()), epoch)\n",
    "        writer.add_scalar(\"loss/actor_sil\", float(sil_loss) if isinstance(sil_loss, float) else float(sil_loss.item()), epoch)\n",
    "        writer.add_scalar(\"loss/value\", float(value_loss.item()), epoch)\n",
    "        writer.add_scalar(\"stats/entropy\", float(entropy.item()), epoch)\n",
    "        writer.add_scalar(\"stats/adv_mean\", float(advantage.mean().item()), epoch)\n",
    "        writer.add_scalar(\"stats/adv_std\", float(advantage.std().item()), epoch)\n",
    "        writer.add_scalar(\"best_reward\", best_reward, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            tag = \"COLLAPSE\" if is_collapse else (\"SPIKE\" if is_spike else \"normal\")\n",
    "            print(\n",
    "                f\"Epoch {epoch:05d} | ep_reward {episode_reward:.1f} | \"\n",
    "                f\"baseline_prev {baseline_prev:.2f} -> ema {baseline:.2f} | \"\n",
    "                f\"{tag} | extra_upd={extra_updates}\"\n",
    "            )\n",
    "            torch.save(policy.state_dict(), \"lstm-policy.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
